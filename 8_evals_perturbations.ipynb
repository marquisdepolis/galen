{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Market and Economic Factors, Perturbation: Competitive Reaction, Description: Considering the market entry of a new competitor\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def get_random_perturbation(perturbations):\n",
    "    category = random.choice(list(perturbations.keys()))\n",
    "    perturbation = random.choice(list(perturbations[category].items()))\n",
    "    return category, perturbation\n",
    "\n",
    "# Usage\n",
    "file_path = 'perturbations.json'\n",
    "perturbations = load_file(file_path)\n",
    "random_category, random_perturbation = get_random_perturbation(perturbations)\n",
    "print(f\"Category: {random_category}, Perturbation: {random_perturbation[0]}, Description: {random_perturbation[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM dynamic evals\n",
    "\n",
    "import replicate\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "folder_path = 'files'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "config = load_file('config.json')\n",
    "\n",
    "INSTRUCTION = config['instructions']\n",
    "F_NAME = config[\"name\"]\n",
    "\n",
    "# # Load the file\n",
    "df = pd.read_excel('files/questions_dynamic.xlsx')\n",
    "# Save the original DataFrame\n",
    "df.to_excel('files/questions_original_dynamic.xlsx', index=False)\n",
    "\n",
    "# Trim whitespace and newline characters\n",
    "df['Question'] = df['Question'].str.strip()  # Removes leading/trailing whitespace\n",
    "\n",
    "# Check for duplicate questions\n",
    "duplicates = df.duplicated(subset=['Question'], keep=False)\n",
    "if duplicates.any():\n",
    "    print(\"Duplicates found. Removing duplicates.\")\n",
    "\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['Question'], keep='first')\n",
    "\n",
    "    # Save the modified DataFrame, overwriting the original 'questions.xlsx'\n",
    "    df.to_excel('files/questions_dynamic.xlsx', index=False)\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "perturbations = load_file('perturbations.json')\n",
    "knowledgebase = load_file('knowledgebase.json')\n",
    "\n",
    "# Modify DataFrame to include new columns\n",
    "results_df = pd.DataFrame(columns=['Model', 'Question', 'Response', 'Perturbed Question', 'Perturbed Response', 'Final Analysis Question', 'Final Analysis Response'])\n",
    "\n",
    "models = {\n",
    "    \"qwen-14b\": \"nomagick/qwen-14b-chat:f9e1ed25e2073f72ff9a3f46545d909b1078e674da543e791dec79218072ae70\",\n",
    "    \"falcon-40b\": \"joehoover/falcon-40b-instruct:7d58d6bddc53c23fa451c403b2b5373b1e0fa094e4e0d1b98c3d02931aa07173\",\n",
    "    \"yi-34b\": \"01-ai/yi-34b-chat:914692bbe8a8e2b91a4e44203e70d170c9c5ccc1359b283c84b0ec8d47819a46\",\n",
    "    \"mistral-7b\": \"mistralai/mistral-7b-instruct-v0.2:f5701ad84de5715051cb99d550539719f8a7fbcf65e0e62a3d1eb3f94720764e\",\n",
    "    \"llama2-70b\": \"meta/llama-2-70b-chat\",\n",
    "    \"openhermes2\": \"antoinelyset/openhermes-2.5-mistral-7b:d7ccd25700fb11c1787c25b580ac8d715d2b677202fe54b77f9b4a1eb7d73e2b\",\n",
    "    \"mixtral-instruct\": \"mistralai/mixtral-8x7b-instruct-v0.1:2b56576fcfbe32fa0526897d8385dd3fb3d36ba6fd0dbe033c72886b81ade93e\",\n",
    "    \"deepseek_33bq\": \"kcaverly/deepseek-coder-33b-instruct-gguf:ea964345066a8868e43aca432f314822660b72e29cab6b4b904b779014fe58fd\",\n",
    "    }\n",
    "\n",
    "prompt_for_qwen=\"\"\"<|im_start|>system\\n {INSTRUCTION}. Please try your best to answer the following question. <|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "prompt_for_hermes = \"\"\"[\n",
    "{{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"{INSTRUCTION}. Please try your best to answer the following question.\" \n",
    "}},\n",
    "{{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": {question}\n",
    "}}\n",
    "]\"\"\"\n",
    "\n",
    "def ask_llm(model_value, prompt):   \n",
    "    output = replicate.run(\n",
    "        model_value,\n",
    "        input={\n",
    "            \"debug\": False,\n",
    "        #   \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\": 500,\n",
    "            \"min_new_tokens\": -1\n",
    "        }\n",
    "    )\n",
    "    response = \"\"\n",
    "    for item in output:\n",
    "        item_str = str(item)  # Convert item to string\n",
    "        response += item_str if len(item_str) == 1 else f\" {item_str}\"\n",
    "    response = response.strip()\n",
    "    return response\n",
    "\n",
    "# Iterate through each model\n",
    "for model_key, model_value in models.items():\n",
    "    responses = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        qn = row['Question']\n",
    "        question = json.dumps(qn)\n",
    "\n",
    "        if model_key == \"yi-34b\":  # Yi model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        if model_key == \"qwen-14b\":  # Qwen model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        elif model_key == \"openhermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        else:\n",
    "            plain_text_question = json.loads(question)\n",
    "            prompt = f\"{INSTRUCTION}. Please try your best to answer the following question. {plain_text_question}\"\n",
    "\n",
    "        try:\n",
    "            print(prompt)\n",
    "            response = ask_llm(model_value, prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "\n",
    "        # Perturb the question and get the response\n",
    "        category, (perturbation, description) = get_random_perturbation(perturbations)\n",
    "        perturbed_qn = f\"{qn}\\nResponse: {response}\\nChange in circumstances: {perturbation} - {description}\\n What should change in the response?\"\n",
    "        if model_key == \"yi-34b\":  # Yi model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=perturbed_qn)\n",
    "        if model_key == \"qwen-14b\":  # Qwen model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=perturbed_qn)\n",
    "        elif model_key == \"openhermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, question=perturbed_qn)\n",
    "        else:\n",
    "            plain_text_question = json.loads(perturbed_qn)\n",
    "            prompt = f\"{INSTRUCTION}. Please try your best to answer the following question. {plain_text_question}\"\n",
    "        perturbed_response = ask_llm(model_value, perturbed_qn)\n",
    "\n",
    "        # Evaluate sufficiency or suggest alternate course\n",
    "        final_analysis_qn = f\"Original Question: {question}\\nOriginal Response: {response} \\nPerturbation ({category}): {perturbation} - {description}\\n {perturbed_response}\\nKnowledgebase: {knowledgebase}\\nNow consider the knowlegebase, what else ought we to do?\"\n",
    "        if model_key == \"yi-34b\":  # Yi model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=final_analysis_qn)\n",
    "        if model_key == \"qwen-14b\":  # Qwen model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=final_analysis_qn)\n",
    "        elif model_key == \"openhermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, question=final_analysis_qn)\n",
    "        else:\n",
    "            plain_text_question = json.loads(final_analysis_qn)\n",
    "            prompt = f\"{INSTRUCTION}. Please try your best to answer the following question. {plain_text_question}\"\n",
    "        final_analysis_response = ask_llm(model_value, final_analysis_qn)\n",
    "\n",
    "        # Record each stage\n",
    "        new_row = {\n",
    "            'Model': model_key, \n",
    "            'Question': qn, \n",
    "            'Response': response, \n",
    "            'Perturbed Question': perturbed_qn, \n",
    "            'Perturbed Response': perturbed_response, \n",
    "            'Final Analysis Question': final_analysis_qn, \n",
    "            'Final Analysis Response': final_analysis_response\n",
    "        }\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        if index % 10 == 0:\n",
    "            results_df.to_excel(f'files/{F_NAME}_results_grouped_by_model_dynamic.xlsx', index=False, sheet_name='Sheet1')\n",
    "            \n",
    "results_df.to_excel(f'files/{F_NAME}_results_grouped_by_model_dynamic.xlsx', index=False, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 dynamic evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "folder_path = 'files'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "config = load_file('config.json')\n",
    "perturbations = load_file('perturbations.json')\n",
    "knowledgebase = load_file('knowledgebase.json')\n",
    "\n",
    "INSTRUCTION = config['instructions']\n",
    "F_NAME = config[\"name\"]\n",
    "\n",
    "GPT_MODEL = \"gpt-4-1106-preview\"\n",
    "INPUT_CSV_PATH = 'files/questions_dynamic.xlsx'\n",
    "OUTPUT_CSV_PATH = f'files/{F_NAME}_results_gpt4_dynamic.xlsx'\n",
    "\n",
    "client = OpenAI()\n",
    "def show_json(obj):\n",
    "    print(json.loads(obj.model_dump_json()))\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=f\"{F_NAME} AI Dynamic Evaluator\",\n",
    "    instructions=INSTRUCTION,\n",
    "    model=GPT_MODEL,\n",
    ")\n",
    "show_json(assistant)\n",
    "\n",
    "# Utility functions\n",
    "def read_csv(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def process_data_for_gpt(data):\n",
    "    prompts = []\n",
    "    for _, row in data.iterrows():\n",
    "        question = row['Question']\n",
    "        prompt = f\"Please try your best to answer the following question.:\\n\\n{question}\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def submit_message_and_create_run(assistant_id, prompt):\n",
    "    thread = client.beta.threads.create() # If you replace this globally it appends all answers to the one before.\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=prompt)\n",
    "    return client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id), thread\n",
    "\n",
    "def wait_on_run_and_get_response(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "        time.sleep(0.5)\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")\n",
    "    return [m.content[0].text.value for m in messages if m.role == 'assistant']\n",
    "\n",
    "data = read_csv(INPUT_CSV_PATH)\n",
    "prompts = process_data_for_gpt(data)\n",
    "ASSISTANT_ID = assistant.id\n",
    "\n",
    "def ask_gpt4(prompt, ASSISTANT_ID):\n",
    "    run, thread = submit_message_and_create_run(ASSISTANT_ID, prompt)\n",
    "    response = wait_on_run_and_get_response(run, thread)\n",
    "    if isinstance(response, list):\n",
    "        response = ' '.join(map(str, response))\n",
    "    response = response.replace(\"\\\\\\\\n\", \"\\\\n\")\n",
    "    response = response.strip()\n",
    "    print(response)\n",
    "    responses.append(response)\n",
    "\n",
    "def process_question_with_gpt4(original_question, assistant_id):\n",
    "    # Get initial response\n",
    "    first_response = ask_gpt4(original_question,assistant_id = ASSISTANT_ID)\n",
    "    # Perturb the question and get the response\n",
    "    category, (perturbation, description) = get_random_perturbation(perturbations)\n",
    "    perturbed_qn = f\"{original_question}\\nResponse: {first_response}\\nChange in circumstances: {perturbation} - {description}\\n What should change in the response?\"\n",
    "    perturbed_response = ask_gpt4(perturbed_qn)\n",
    "\n",
    "    # Evaluate sufficiency or suggest alternate course\n",
    "    final_analysis_qn = f\"Original Question: {question}\\nOrig Response: {first_response} \\nPerturbation ({category}): {perturbation} - {description}\\n {perturbed_response}\\nKnowledgebase Content: {knowledgebase}\\n Now consider the knowlegebase, what else ought we to do?\"\n",
    "    final_analysis_response = ask_gpt4(final_analysis_qn)\n",
    "\n",
    "    return {\n",
    "        'Question': original_question, \n",
    "        'Response': first_response, \n",
    "        'Perturbed Question': perturbed_qn, \n",
    "        'Perturbed Response': perturbed_response, \n",
    "        'Final Analysis Question': final_analysis_qn, \n",
    "        'Final Analysis Response': final_analysis_response\n",
    "    }\n",
    "\n",
    "# Modify DataFrame to include new columns\n",
    "new_data_columns = ['Model', 'Question', 'Response', 'Perturbed Question', 'Perturbed Response', 'Final Analysis Question', 'Final Analysis Response']\n",
    "results_df = pd.DataFrame(columns=new_data_columns)\n",
    "\n",
    "# Process each question\n",
    "for prompt in prompts:\n",
    "    processed_info = process_question_with_gpt4(prompt, ASSISTANT_ID)\n",
    "    results_df = results_df.append(processed_info, ignore_index=True)\n",
    "\n",
    "# Save the results\n",
    "results_df.to_excel(OUTPUT_CSV_PATH, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
