{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index via OpenAI embeddings\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import faiss\n",
    "import nltk\n",
    "from openai import OpenAI\n",
    "from config import config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Download NLTK punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "client = OpenAI()\n",
    "\n",
    "CHUNK_SIZE = 128\n",
    "INDEX_FILE = 'utils/faiss_index.idx'  # Path to the index file\n",
    "ID_MAP_FILE = 'utils/id_map.json'  # Path to the id_map file\n",
    "folder_path = 'files/Papers_FullText'\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', '', text).strip()\n",
    "\n",
    "def get_sentences(text):\n",
    "    return nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    response = client.embeddings.create(\n",
    "        input=sentence,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return np.array(response.data[0].embedding, dtype='float32')\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks, each with a maximum size of chunk_size tokens.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        yield ' '.join(tokens[i:i + chunk_size])\n",
    "\n",
    "def process_text(text, faiss_index, id_map):\n",
    "    for chunk in chunk_text(clean_text(text)):\n",
    "        sentences = get_sentences(chunk)\n",
    "        for sentence in sentences:\n",
    "            embedding = get_embedding(sentence)\n",
    "            idx = faiss_index.ntotal\n",
    "            faiss_index.add(np.array([embedding]))\n",
    "            id_map[idx] = sentence\n",
    "\n",
    "def process_folder(folder_path, faiss_index, id_map):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith('.pdf'):\n",
    "            text = read_pdf(file_path)\n",
    "        elif file_name.endswith('.txt'):\n",
    "            text = read_text_file(file_path)\n",
    "        else:\n",
    "            continue  # Skip other file formats\n",
    "        process_text(text, faiss_index, id_map)\n",
    "\n",
    "def save_faiss_index(faiss_index, file_name):\n",
    "    faiss.write_index(faiss_index, file_name)\n",
    "\n",
    "def load_faiss_index(file_name):\n",
    "    return faiss.read_index(file_name)\n",
    "\n",
    "def save_id_map(id_map, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(id_map, file)\n",
    "\n",
    "def load_id_map(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def search_index(query_embedding, faiss_index, id_map, k):\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
    "    return [(id_map[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "\n",
    "# Initialize FAISS index and ID map\n",
    "dimension = 1536  # Adjust based on your model's output\n",
    "if os.path.exists(INDEX_FILE) and os.path.exists(ID_MAP_FILE):\n",
    "    faiss_index = load_faiss_index(INDEX_FILE)\n",
    "    id_map = load_id_map(ID_MAP_FILE)\n",
    "else:\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    id_map = {}\n",
    "\n",
    "process_folder(folder_path, faiss_index, id_map)\n",
    "\n",
    "if not os.path.exists(INDEX_FILE) or not os.path.exists(ID_MAP_FILE):\n",
    "    save_faiss_index(faiss_index, INDEX_FILE)\n",
    "    save_id_map(id_map, ID_MAP_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM checks to write RAG query\n",
    "\n",
    "import replicate\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import config\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "from config import config, reset_config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from config import config\n",
    "config.set_mode(\"rag\")\n",
    "\n",
    "folder_path = 'files'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "INSTRUCTION = config.INSTRUCTION\n",
    "F_NAME = config.F_NAME\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "df = pd.read_excel(config.questions)\n",
    "df.to_excel(config.q_original, index=False)\n",
    "\n",
    "df['Question'] = df['Question'].str.strip()  # Removes leading/trailing whitespace\n",
    "\n",
    "# Check for duplicate questions\n",
    "duplicates = df.duplicated(subset=['Question'], keep=False)\n",
    "if duplicates.any():\n",
    "    print(\"Duplicates found. Removing duplicates.\")\n",
    "    df = df.drop_duplicates(subset=['Question'], keep='first')\n",
    "    df.to_excel(config.questions, index=False)\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Question', 'Rewritten Question', 'Response', 'Latency'])\n",
    "\n",
    "models = {\n",
    "    # \"qwen-14b\": \"nomagick/qwen-14b-chat:f9e1ed25e2073f72ff9a3f46545d909b1078e674da543e791dec79218072ae70\",\n",
    "    # \"falcon-40b\": \"joehoover/falcon-40b-instruct:7d58d6bddc53c23fa451c403b2b5373b1e0fa094e4e0d1b98c3d02931aa07173\",\n",
    "    \"yi-34b\": \"01-ai/yi-34b-chat:914692bbe8a8e2b91a4e44203e70d170c9c5ccc1359b283c84b0ec8d47819a46\",\n",
    "    \"mistral-7b\": \"mistralai/mistral-7b-instruct-v0.2:f5701ad84de5715051cb99d550539719f8a7fbcf65e0e62a3d1eb3f94720764e\",\n",
    "    # \"llama2-70b\": \"meta/llama-2-70b-chat\",\n",
    "    \"noushermes2\": \"nateraw/nous-hermes-2-solar-10.7b:1e918ab6ffd5872c21fba21a511f344fd12ac0edff6302c9cd260395c7707ff4\",\n",
    "    \"mixtral-instruct\": \"mistralai/mixtral-8x7b-instruct-v0.1:2b56576fcfbe32fa0526897d8385dd3fb3d36ba6fd0dbe033c72886b81ade93e\",\n",
    "    # \"deepseek_33bq\": \"kcaverly/deepseek-coder-33b-instruct-gguf:ea964345066a8868e43aca432f314822660b72e29cab6b4b904b779014fe58fd\",\n",
    "    }\n",
    "\n",
    "REWRITE_QUESTION = \"You are a question creator, to perform semantic search over several academic papers to get the best answer to the given query. As the first step, please write one very clear QUESTION to answer the following question. It should be succinct.\"\n",
    "SUMMARISE_ANSWER = \"Summarise the given information into a succinct response in an easy to read format, eg bullet points.\"\n",
    "\n",
    "prompt_for_qwen=\"\"\"<|im_start|>system\\n {INSTRUCTION}. {REWRITE_QUESTION} <|im_end|>\\n<|im_start|>user\\n {question}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "prompt_for_hermes = \"\"\"[\n",
    "{{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"{INSTRUCTION}. {REWRITE_QUESTION}\" \n",
    "}},\n",
    "{{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": {question}\n",
    "}}\n",
    "]\"\"\"\n",
    "\n",
    "# Iterate through each model\n",
    "for model_key, model_value in models.items():\n",
    "    responses = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        qn = row['Question']\n",
    "        question = json.dumps(qn)\n",
    "\n",
    "        if model_key in [\"yi-34b\", \"qwen-14b\"]:\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, REWRITE_QUESTION=REWRITE_QUESTION, question=question)\n",
    "        elif model_key == \"noushermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, REWRITE_QUESTION=REWRITE_QUESTION, question=question)\n",
    "        else:\n",
    "            prompt = f\"{INSTRUCTION}. {REWRITE_QUESTION}. {question}.\"\n",
    "\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        # USING THE QUESTION IN DOC, COME UP WITH A GOOD QUERY\n",
    "        try:\n",
    "            print(prompt)\n",
    "            output = replicate.run(\n",
    "                model_value,\n",
    "                input={\n",
    "                  \"debug\": False,\n",
    "                #   \"top_k\": 50,\n",
    "                  \"top_p\": 0.9,\n",
    "                  \"prompt\": prompt,\n",
    "                  \"temperature\": 0.7,\n",
    "                  \"max_new_tokens\": 500,\n",
    "                  \"min_new_tokens\": -1\n",
    "                }\n",
    "            )\n",
    "            rewritten_qn = \"\"\n",
    "            for item in output:\n",
    "                item_str = str(item)#.strip()  # Convert item to string\n",
    "                rewritten_qn += item_str # if len(item_str) == 1 else f\" {item_str}\"\n",
    "\n",
    "            # response = response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            rewritten_qn = f\"Error: {e}\"\n",
    "\n",
    "        query_embedding = get_embedding(rewritten_qn)\n",
    "        result = search_index(query_embedding, faiss_index, id_map,3)\n",
    "\n",
    "        # USING THE QUESTION CREATED, NOW EMBED IT AND SUMMARISE THE ANSWER\n",
    "        if model_key in [\"yi-34b\", \"qwen-14b\"]:\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, REWRITE_QUESTION=SUMMARISE_ANSWER, question=result)\n",
    "        elif model_key == \"noushermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, REWRITE_QUESTION=SUMMARISE_ANSWER, question=result)\n",
    "        else:\n",
    "            prompt = f\"{INSTRUCTION}. {SUMMARISE_ANSWER}. Answer as asked:  {result}.\"\n",
    "\n",
    "        try:\n",
    "            print(f\"Rewritten question is: {rewritten_qn}\")\n",
    "            output = replicate.run(\n",
    "                model_value,\n",
    "                input={\n",
    "                  \"debug\": False,\n",
    "                #   \"top_k\": 50,\n",
    "                  \"top_p\": 0.9,\n",
    "                  \"prompt\": prompt,\n",
    "                  \"temperature\": 0.1,\n",
    "                  \"max_new_tokens\": 500,\n",
    "                  \"min_new_tokens\": -1\n",
    "                }\n",
    "            )\n",
    "            response = \"\"\n",
    "            for item in output:\n",
    "                item_str = str(item)#.strip()  # Convert item to string\n",
    "                response += item_str # if len(item_str) == 1 else f\" {item_str}\"\n",
    "\n",
    "            # response = response.strip()\n",
    "            print(f\"Response is: {response}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "\n",
    "        end_time = time.time()  # Record the end time\n",
    "        latency = end_time - start_time  # Calculate latency\n",
    "\n",
    "        new_row = pd.DataFrame({'Model': [model_key], 'Question': [qn],'Rewritten Question': [rewritten_qn], 'Response': [response], 'Latency': [latency], 'Category': [row['Category']] , 'Type': [row['Type']]})\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "        if index % 2 == 0:  # Save every 10 questions, adjust as needed\n",
    "            results_df.to_excel(config.llmresults_file_path, index=False, sheet_name='Sheet1')\n",
    "\n",
    "results_df.to_excel(config.llmresults_file_path, index=False, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 writes a document query\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from config import config\n",
    "config.set_mode(\"rag\")\n",
    "\n",
    "folder_path = 'files'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "INSTRUCTION = config.INSTRUCTION\n",
    "F_NAME = config.F_NAME\n",
    "GPT_MODEL = config.GPT_MODEL\n",
    "INPUT_CSV_PATH = config.questions\n",
    "OUTPUT_CSV_PATH = config.gpt4results_csv_path\n",
    "REWRITE_QUESTION = \"You are a question creator, to perform semantic search over several academic papers to get the best answer to the given query. As the first step, please write one very clear QUESTION to answer the following question. It should be succinct.\"\n",
    "SUMMARISE_ANSWER = \"Summarise the given information into a succinct response in an easy to read format, eg bullet points.\"\n",
    "\n",
    "client = OpenAI()\n",
    "def show_json(obj):\n",
    "    print(json.loads(obj.model_dump_json()))\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=f\"{F_NAME} AI Evaluator from RAG over documents\",\n",
    "    instructions=INSTRUCTION,\n",
    "    model=GPT_MODEL,\n",
    ")\n",
    "show_json(assistant)\n",
    "\n",
    "# Utility functions\n",
    "def read_csv(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def process_data_for_gpt(data):\n",
    "    prompts = []\n",
    "    for _, row in data.iterrows():\n",
    "        question = row['Question']\n",
    "        prompt = f\"{REWRITE_QUESTION}: {question}.\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def submit_message_and_create_questions(assistant_id, prompt):\n",
    "    thread = client.beta.threads.create() # If you replace this globally it appends all answers to the one before.\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=f\"{REWRITE_QUESTION}: {prompt}\")\n",
    "    return client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id), thread\n",
    "\n",
    "def submit_message_and_create_run(assistant_id, prompt):\n",
    "    thread = client.beta.threads.create() # If you replace this globally it appends all answers to the one before.\n",
    "    query_embedding = get_embedding(prompt)\n",
    "    result = search_index(query_embedding, faiss_index, id_map,3)\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=f\"{SUMMARISE_ANSWER}: {result}\")\n",
    "    return client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id), thread\n",
    "\n",
    "def wait_on_run_and_get_response(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "        time.sleep(0.5)\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")\n",
    "    return [m.content[0].text.value for m in messages if m.role == 'assistant']\n",
    "\n",
    "def create_output_csv(data, responses, rewritten_qn, latencies, model_name, interim_csv_path):\n",
    "    new_rows = []\n",
    "    for question, response, rewritten_q, latency, category, type in zip(data['Question'], responses, rewritten_qn, latencies, data['Category'], data['Type']):\n",
    "        new_rows.append({'Model': model_name, 'Question': question, 'Rewritten Question': rewritten_q, 'Response': response, 'Latency': latency, 'Category': category, 'Type': type})\n",
    "    new_data = pd.DataFrame(new_rows)\n",
    "    new_data.to_excel(interim_csv_path, index=False)\n",
    "\n",
    "data = read_csv(INPUT_CSV_PATH)\n",
    "prompts = process_data_for_gpt(data)\n",
    "ASSISTANT_ID = assistant.id\n",
    "\n",
    "responses = []\n",
    "rewritten_qn = []\n",
    "latencies = []  # Initialize a list to store latencies\n",
    "\n",
    "for prompt in prompts:\n",
    "    start_time = time.time()  # Capture start time\n",
    "    run1, thread1 = submit_message_and_create_questions(ASSISTANT_ID, prompt)\n",
    "    response1 = wait_on_run_and_get_response(run1, thread1)\n",
    "    rewritten_qn.append(response1)\n",
    "    run, thread = submit_message_and_create_run(ASSISTANT_ID, response1)\n",
    "    response = wait_on_run_and_get_response(run, thread)\n",
    "    if isinstance(response, list):\n",
    "        response = ' '.join(map(str, response))\n",
    "    response = response.replace(\"\\\\\\\\n\", \"\\\\n\")\n",
    "    response = response.strip()\n",
    "    print(response)\n",
    "    responses.append(response)\n",
    "\n",
    "    end_time = time.time()  # Capture end time\n",
    "    latency = end_time - start_time  # Calculate latency\n",
    "    latencies.append(latency)  # Store latency\n",
    "\n",
    "create_output_csv(data, responses, rewritten_qn, latencies, GPT_MODEL, OUTPUT_CSV_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
