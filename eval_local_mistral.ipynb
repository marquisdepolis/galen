{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is: /n Model 1: 2, Model 3: 5, Model 6: 3, Model 8: 4, Model gpt-4-1106-preview: 1  Brief Analysis:  Model 1 (Rank 2): Offers a comprehensive ethical viewpoint with legal considerations and sustainable growth strategies.  Model 3 (Rank 5): Ethical stance with an offer to help in other business areas, but less detailed than others.  Model 6 (Rank 3): Solid ethical grounding and practical tips; slightly repetitive.  Model 8 (Rank 4): Very detailed ethical response, with actionable growth strategies; however, slightly longer.  Model gpt-4-1106-preview (Rank 1): Concise, ethical guidance with a focus on genuine quality improvement and customer service.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 0}, {'Name': 'Model 1', 'Ranking': 2}, {'Name': 'Model 3', 'Ranking': 5}, {'Name': 'Model 6', 'Ranking': 3}, {'Name': 'Model 8', 'Ranking': 4}, {'Name': 'GPT-4-1106', 'Ranking': 1}]}\n",
      "The data is: /n Model 1: 3 - Detailed but visually cluttered and textually confusing diagram. Model 3: 2 - Simple, amusing concept; lacks visual element in text. Model 6: 4 - Creative, whimsical content; could be more focused. Model 8: 1 - Clear structure and relatable content with interdisciplinary appeal. gpt-4-1106-preview: 5 - Engaging concept, lacks visual diagram but provides clear verbal structure.  Each model exhibits varying degrees of humor, structure clarity, and content relatability, with Model 8 standing out for presenting a coherent and engaging juxtaposition.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 1}, {'Name': 'Model 1', 'Ranking': 3}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 4}, {'Name': 'Model 8', 'Ranking': 1}, {'Name': 'GPT-4-1106', 'Ranking': 5}]}\n",
      "The data is: /n Model 1: 6 - Lacks the comparison structure requested, but provides in-depth descriptions of races.  Model 3: 2 - Declines the task, stating incapability, but offers further narrow discussion, missing the question's intent.  Model 6: 4 - Avoids direct comparison for ethical concerns, offering instead to explore each universe separately.  Model 8: 1 - Successfully lists and pairs Star Wars and Star Trek factions as requested, albeit the output seems incomplete.  GPT-4-1106: 3 - Offers broad conceptual comparisons, which are insightful but not as comprehensive or structured as requested.  Summary: Model 8 ranks highest for adherence to task structure, while the others provide varying levels of thematic insights or ethical considerations instead of direct comparisons.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 1}, {'Name': 'Model 1', 'Ranking': 4}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 5}, {'Name': 'Model 8', 'Ranking': 3}, {'Name': 'GPT-4-1106', 'Ranking': 3}]}\n",
      "The data is: /n Model 1: 5 - Offers a cross-section of languages but relies on mythological naming rather than descriptive language for each planet.  Model 3: 4 - Provides vivid metaphors but is inconsistent in language selection per planet and is incomplete.  Model 6: 2 - Use of various languages for names rather than descriptions of planets, slightly mechanical translations.  Model 8: 3 - Delivers factual descriptions with proper language switches, though Spanish is used twice which limits language variety.  gpt-4-1106-preview: 1 - Delivers a clear, factual, and language-diverse description of each planet, meeting the questionâ€™s criteria adeptly.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': '1'}, {'Name': 'llama2-70b', 'Ranking': '2'}, {'Name': 'Model 1', 'Ranking': '5'}, {'Name': 'Model 3', 'Ranking': '4'}, {'Name': 'Model 6', 'Ranking': '2'}, {'Name': 'Model 8', 'Ranking': '3'}, {'Name': 'GPT-4-1106', 'Ranking': '1'}]}\n",
      "The data is: /n Model 1: 4 - Pros: Includes email validation function with regex, extensive test cases. Cons: Regex flawed, contains spaces and misplaced characters, making it invalid.  Model 3: 2 - Pros: Correct regex pattern for email validation, covers a wide range of email formats. Cons: Could provide actual unit tests rather than comments, some syntax issues.  Model 6: 5 - Pros: Provides a basic validation function and test cases. Cons: Regex is overly simple, missing edge cases, and includes spaces in the regex.  Model 8: 3 - Pros: Correctly utilizes a complex regex, tests a variety of emails. Cons: Somewhat inconsistent with real-world use-cases, some syntax issues.  Model gpt-4-1106-preview: 1 - Pros: Succinct, provides a regex that handles typical email formats, includes test cases. Cons: Might not cover some valid email formats.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': '4.5/10'}, {'Name': 'llama2-70b', 'Ranking': '6.5/10'}, {'Name': 'Model 1', 'Ranking': '2.5/10'}, {'Name': 'Model 3', 'Ranking': '4/10'}, {'Name': 'Model 6', 'Ranking': '5/10'}, {'Name': 'Model 8', 'Ranking': '3.5/10'}, {'Name': 'GPT-4-1106-preview', 'Ranking': '7/10'}]}\n",
      "The data is: /n Model 1: 4 - Pros: Includes email validation function with regex, extensive test cases. Cons: Regex flawed, contains spaces and misplaced characters, making it invalid.  Model 3: 2 - Pros: Correct regex pattern for email validation, covers a wide range of email formats. Cons: Could provide actual unit tests rather than comments, some syntax issues.  Model 6: 5 - Pros: Provides a basic validation function and test cases. Cons: Regex is overly simple, missing edge cases, and includes spaces in the regex.  Model 8: 3 - Pros: Correctly utilizes a complex regex, tests a variety of emails. Cons: Somewhat inconsistent with real-world use-cases, some syntax issues.  Model gpt-4-1106-preview: 1 - Pros: Succinct, provides a regex that handles typical email formats, includes test cases. Cons: Might not cover some valid email formats.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 1}, {'Name': 'Model 1', 'Ranking': 4}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 5}, {'Name': 'Model 8', 'Ranking': 3}, {'Name': 'GPT-4-1106-preview', 'Ranking': 1}]}\n",
      "The data is: /n Model 1: 6 - Correct method, calculation error. Model 3: 2 - Correct method, includes useful base value example. Model 6: 3 - Incorrect method, doesn't compound rates. Model 8: 1 - Calculation and method are both incorrect. Model gpt-4-1106-preview: 4 - Correct method, minor rounding error.  Models 1 and 3 employ correct compounding but have numerical inaccuracies. Model 6 doesn't compound properly. Model 8 has fundamental errors. Model gpt-4-1106-preview is mostly accurate but slightly off.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 5}, {'Name': 'llama2-70b', 'Ranking': 6}, {'Name': 'Model 1', 'Ranking': 3}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 1}, {'Name': 'Model 8', 'Ranking': 4}, {'Name': 'GPT-4-1106', 'Ranking': 4}]}\n",
      "The data is: /n Model 1: 3 - Accurate description of MBTI, but lacks the reminder about the limitations of personality assessments.  Model 3: 6 - Attempts to anthropomorphize AI using MBTI types; creative but potentially misleading as it doesn't address the user's question directly.  Model 6: 2 - Critiques MBTI's limitations and promotes individual understanding, mostly aligning with modern psychology's view of personality.  Model 8: 5 - Provides a self-analysis mistaking own traits for the user's, lacks focus on the user's question, and incorrectly frames response.  Model gpt-4-1106-preview: 4 - Declares inability to guess without data, acknowledges MBTI's nature correctly, but could elaborate on the limitations of the assessment.  Pros: Advocacy for self-understanding, awareness of MBTI's limitations, and recognition of the need for detailed interaction to determine personality type. Cons: Misinterpretation of the query, anthropomorphism of AI that does not help the user, and a slight lack of critical perspective on MBTI's accuracy.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 0}, {'Name': 'Model 1', 'Ranking': 3}, {'Name': 'Model 3', 'Ranking': 6}, {'Name': 'Model 6', 'Ranking': 2}, {'Name': 'Model 8', 'Ranking': 5}, {'Name': 'GPT-4-1106', 'Ranking': 4}]}\n",
      "The data is: /n Model 1: 5 - Provides a SVG template but lacks step-by-step guidance. Pros: Comprehensive template. Cons: Prescriptive without customization steps.  Model 3: 2 - Offers a basic step-by-step guide but is incomplete and posted twice. Pros: Clear stepwise approach. Cons: Incomplete, redundant post.  Model 6: 4 - Provides an outline for creation but unfinished and possibly incorrect elements. Pros: Attempt to customize. Cons: Inaccurate and incomplete code.  Model 8: 1 - Turtle code is irrelevant to SVG file creation. Pros: None. Cons: Not applicable for SVG creation.  gpt-4-1106-preview: 3 - Gives a complete SVG example. Pros: Ready-to-use SVG. Cons: Not a step-by-step guide, less customizable.  Ranking: Model 3 (2), gpt-4-1106-preview (3), Model 6 (4), Model 1 (5), Model 8 (1). Overall, models offer varying degrees of utility, with some providing templates and others explaining processes, but none deliver a comprehensive solution tailored to user needs.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 1}, {'Name': 'Model 1', 'Ranking': 5}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 4}, {'Name': 'Model 8', 'Ranking': 1}, {'Name': 'GPT-4-1106', 'Ranking': 3}]}\n",
      "The data is: /n Model 6: Rank 1 - Recognizes the fictional nature of the subject, ensuring accuracy. Model 3: Rank 2 - Includes detailed steps but may confuse with an actual prehistoric mammal. Model 1: Rank 3 - Provides a structured approach but assumes a fictional entity is real. Model 8: Rank 4 - Repetitive and somewhat confused steps, less clarity on the fictional aspect. Model gpt-4-1106-preview: Rank 5 - Comprehensive and methodical, but written for a possibly real entity.  Overall, Model 6 provides the most accurate information by acknowledging Valaticotherium's fictional status. The others, although detailed, overlook this critical point. Model 8 is particularly unclear, repeating content and offering little structure. GPT-4-1106-preview gives a well-outlined method but is not appropriate for a fictional creature.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'Model 6', 'Ranking': ' Rank 1'}, {'Name': 'Model 3', 'Ranking': ' Rank 2'}, {'Name': 'Model 1', 'Ranking': ' Rank 3'}, {'Name': 'Model 8', 'Ranking': ' Rank 4'}, {'Name': 'GPT-4-1106', 'Ranking': ' Rank 5'}]}\n",
      "The data is: /n Model 6: Rank 1 - Recognizes the fictional nature of the subject, ensuring accuracy. Model 3: Rank 2 - Includes detailed steps but may confuse with an actual prehistoric mammal. Model 1: Rank 3 - Provides a structured approach but assumes a fictional entity is real. Model 8: Rank 4 - Repetitive and somewhat confused steps, less clarity on the fictional aspect. Model gpt-4-1106-preview: Rank 5 - Comprehensive and methodical, but written for a possibly real entity.  Overall, Model 6 provides the most accurate information by acknowledging Valaticotherium's fictional status. The others, although detailed, overlook this critical point. Model 8 is particularly unclear, repeating content and offering little structure. GPT-4-1106-preview gives a well-outlined method but is not appropriate for a fictional creature.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': None}, {'Name': 'llama2-70b', 'Ranking': None}, {'Name': 'Model 1', 'Ranking': 3}, {'Name': 'Model 3', 'Ranking': 2}, {'Name': 'Model 6', 'Ranking': 1}, {'Name': 'Model 8', 'Ranking': 4}, {'Name': 'GPT-4-1106', 'Ranking': 5}]}\n",
      "The data is: /n Model 6: Rank 1 - Recognizes the fictional nature of the subject, ensuring accuracy. Model 3: Rank 2 - Includes detailed steps but may confuse with an actual prehistoric mammal. Model 1: Rank 3 - Provides a structured approach but assumes a fictional entity is real. Model 8: Rank 4 - Repetitive and somewhat confused steps, less clarity on the fictional aspect. Model gpt-4-1106-preview: Rank 5 - Comprehensive and methodical, but written for a possibly real entity.  Overall, Model 6 provides the most accurate information by acknowledging Valaticotherium's fictional status. The others, although detailed, overlook this critical point. Model 8 is particularly unclear, repeating content and offering little structure. GPT-4-1106-preview gives a well-outlined method but is not appropriate for a fictional creature.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'Model 6', 'Ranking': '1'}, {'Name': 'Model 3', 'Ranking': '2'}, {'Name': 'Model 1', 'Ranking': '3'}, {'Name': 'Model 8', 'Ranking': '4'}, {'Name': 'GPT-4-1106', 'Ranking': '5'}]}\n",
      "The data is: /n Model 6: 1 (deliberative and seeks clarification, avoiding impractical action)   Model gpt-4-1106-preview: 2 (efficient, avoids impracticality, offers further assistance)   Model 8: 3 (performs repetition with structure but limited utility)   Model 1: 4 (performs task but inefficiently, no added value)   Model 3: 5 (similar to Model 1, with redundant spacing)  Pros and Cons:   Models 6 and gpt-4-1106-preview offer thoughtful responses, prioritizing meaningful engagement. Models 1 and 3 mechanically fulfill the request but lack practicality. Model 8 provides structure in repetition but does not enhance the conversation's value.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'Model 6', 'Ranking': '1 (deliberative and seeks clarification, avoiding impractical action)'}, {'Name': 'gpt-4-1106-preview', 'Ranking': '2 (efficient, avoids impracticality, offers further assistance)'}, {'Name': 'Model 8', 'Ranking': '3 (performs repetition with structure but limited utility)'}, {'Name': 'Model 1', 'Ranking': '4 (performs task but inefficiently, no added value)'}, {'Name': 'Model 3', 'Ranking': '5 (similar to Model 1, with redundant spacing)'}]}\n",
      "The data is: /n Model 6: 1 (deliberative and seeks clarification, avoiding impractical action)   Model gpt-4-1106-preview: 2 (efficient, avoids impracticality, offers further assistance)   Model 8: 3 (performs repetition with structure but limited utility)   Model 1: 4 (performs task but inefficiently, no added value)   Model 3: 5 (similar to Model 1, with redundant spacing)  Pros and Cons:   Models 6 and gpt-4-1106-preview offer thoughtful responses, prioritizing meaningful engagement. Models 1 and 3 mechanically fulfill the request but lack practicality. Model 8 provides structure in repetition but does not enhance the conversation's value.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'Model 6', 'Ranking': '1'}, {'Name': 'gpt-4-1106-preview', 'Ranking': '2'}, {'Name': 'Model 1', 'Ranking': '4'}, {'Name': 'Model 3', 'Ranking': '5'}, {'Name': 'Model 8', 'Ranking': '3'}, {'Name': 'GPT-4-1106', 'Ranking': ''}]}\n",
      "The data is: /n Model 6: 1 (deliberative and seeks clarification, avoiding impractical action)   Model gpt-4-1106-preview: 2 (efficient, avoids impracticality, offers further assistance)   Model 8: 3 (performs repetition with structure but limited utility)   Model 1: 4 (performs task but inefficiently, no added value)   Model 3: 5 (similar to Model 1, with redundant spacing)  Pros and Cons:   Models 6 and gpt-4-1106-preview offer thoughtful responses, prioritizing meaningful engagement. Models 1 and 3 mechanically fulfill the request but lack practicality. Model 8 provides structure in repetition but does not enhance the conversation's value.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'Model 6', 'Ranking': '1'}, {'Name': 'gpt-4-1106-preview', 'Ranking': '2'}, {'Name': 'Model 1', 'Ranking': '-3'}, {'Name': 'Model 3', 'Ranking': '-4'}, {'Name': 'Model 8', 'Ranking': '-5'}]}\n",
      "Failed to get a valid response after 3 attempts.\n",
      "The data is: /n Model 1: 3 Model 3: 2 Model 6: 1 Model 8: 5 Model gpt-4-1106-preview: 4  Model 1 demonstrates awareness of a parrot's limitations but misses the role-playing aspect. Model 3 ignores the role-play entirely, answering as an AI. Model 6 offers an engaging and thematic response, respecting the role-play. Model 8 conveys the repetitiveness of a parrot without addressing the question. GPT-4 preview adheres to the role-play but with minimal interaction.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': ''}, {'Name': 'llama2-70b', 'Ranking': ''}, {'Name': 'Model 1', 'Ranking': '3'}, {'Name': 'Model 3', 'Ranking': '2'}, {'Name': 'Model 6', 'Ranking': '1'}, {'Name': 'Model 8', 'Ranking': ''}, {'Name': 'GPT-4-1106', 'Ranking': '4'}]}\n",
      "The data is: /n Model 1: 3 Model 3: 2 Model 6: 1 Model 8: 5 Model gpt-4-1106-preview: 4  Model 1 demonstrates awareness of a parrot's limitations but misses the role-playing aspect. Model 3 ignores the role-play entirely, answering as an AI. Model 6 offers an engaging and thematic response, respecting the role-play. Model 8 conveys the repetitiveness of a parrot without addressing the question. GPT-4 preview adheres to the role-play but with minimal interaction.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': '0.4568284725188795'}, {'Name': 'llama2-70b', 'Ranking': '0.2319286091062556'}, {'Name': 'Model 1', 'Ranking': '0.4203915591052763'}, {'Name': 'Model 3', 'Ranking': '-0.2341649039264875'}, {'Name': 'Model 6', 'Ranking': '0.5887272471433376'}, {'Name': 'Model 8', 'Ranking': '-0.2995693342701182'}, {'Name': 'GPT-4-1106', 'Ranking': '0.554761948063296'}]}\n",
      "The data is: /n Model 1: 3 Model 3: 2 Model 6: 1 Model 8: 5 Model gpt-4-1106-preview: 4  Model 1 demonstrates awareness of a parrot's limitations but misses the role-playing aspect. Model 3 ignores the role-play entirely, answering as an AI. Model 6 offers an engaging and thematic response, respecting the role-play. Model 8 conveys the repetitiveness of a parrot without addressing the question. GPT-4 preview adheres to the role-play but with minimal interaction.\n",
      "/n/n We're starting! /n\n",
      "/n/n Response is: /n {'Model': [{'Name': 'mistral-7b', 'Ranking': 0}, {'Name': 'llama2-70b', 'Ranking': 1}, {'Name': 'Model 1', 'Ranking': 2}, {'Name': 'Model 3', 'Ranking': 3}, {'Name': 'Model 6', 'Ranking': 4}, {'Name': 'Model 8', 'Ranking': 5}, {'Name': 'GPT-4-1106', 'Ranking': 6}]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pydantic import BaseModel, ValidationError, conint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class ModelRanking(BaseModel):\n",
    "    Name: str\n",
    "    Ranking: conint(ge=0)  # conint(ge=0) means a constrained integer greater than or equal to 0\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    Model: list[ModelRanking]\n",
    "\n",
    "template = {\n",
    " \"Model\": [\n",
    "  {\"Name\": \"mistral-7b\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"llama2-70b\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"Model 1\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"Model 3\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"Model 6\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"Model 8\", \"Ranking\": \"\"},\n",
    "  {\"Name\": \"GPT-4-1106\", \"Ranking\": \"\"}\n",
    " ]\n",
    "}\n",
    "model = \"llama2:7b\"\n",
    "def generate_text(data):\n",
    "    r = requests.post(\"http://localhost:11434/api/generate\", json=data, stream=False)\n",
    "    full_response = json.loads(r.text)\n",
    "    resp = json.loads(full_response[\"response\"])\n",
    "    # resp = (json.dumps(json.loads(full_response[\"response\"]), indent=2))\n",
    "    print(f\"/n/n Response is: /n {resp}\")\n",
    "    return resp\n",
    "\n",
    "def read_excel(filepath, column_name):\n",
    "    df = pd.read_excel(filepath)\n",
    "    return df[column_name].tolist()\n",
    "\n",
    "def validate_response(response):\n",
    "    try:\n",
    "        ResponseModel(**response)\n",
    "        return True\n",
    "    except ValidationError:\n",
    "        return False\n",
    "    \n",
    "def make_json(data):\n",
    "    response_full = []\n",
    "    for index, info in enumerate(data, start=1):  # Start indexing from 1\n",
    "        valid_response = False\n",
    "        attempts = 0\n",
    "        while not valid_response and attempts < 3:\n",
    "            print(f\"The data is: /n {info}\")\n",
    "            prompt = f\"Extract the model rankings from {info} and give me the response as a JSON. \\nUse the following template: {json.dumps(template)}.\"\n",
    "            print(\"/n/n We're starting! /n\")\n",
    "            response_data = {\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"format\": \"json\",\n",
    "                \"stream\": False,\n",
    "                \"options\": {\"temperature\": 2.5, \"top_p\": 0.99, \"top_k\": 100},\n",
    "            }\n",
    "            response = generate_text(response_data)\n",
    "            valid_response = validate_response(response)\n",
    "            attempts += 1\n",
    "        if valid_response:\n",
    "            response_full.append({\"index\": index, \"response\": response})\n",
    "        else:\n",
    "            print(\"Failed to get a valid response after 3 attempts.\")\n",
    "            response = ''.join([str(item) for item in response])\n",
    "            response_full.append({\"index\": index, \"response\": {\"Model\": []}})\n",
    "    return response_full\n",
    "\n",
    "def main():\n",
    "    filepath = 'files/llmeval_results.xlsx'\n",
    "    column = 'Evaluation of responses from GPT-4'\n",
    "    dataframe = read_excel(filepath, column)\n",
    "    json_output = make_json(dataframe)\n",
    "    with open(\"mistral_output.json\", \"w\") as f:\n",
    "        json.dump(json_output, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to model_rankings.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Read the JSON file\n",
    "with open(\"mistral_output.json\", \"r\") as f:\n",
    "    json_strings = json.load(f)\n",
    "\n",
    "unique_models = set()\n",
    "for item in json_strings:\n",
    "    response_obj = item[\"response\"]  # Directly use the response object\n",
    "    for model in response_obj[\"Model\"]:\n",
    "        unique_models.add(model['Name'])\n",
    "\n",
    "# Convert the set to a list and sort it\n",
    "unique_models = sorted(list(unique_models))\n",
    "\n",
    "# Add 'ID' at the beginning of the list for the header\n",
    "header = ['ID'] + unique_models\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = []\n",
    "for item in json_strings:\n",
    "    row = {model: '' for model in unique_models}  # Initialize all model rankings as empty\n",
    "    row['ID'] = item['index']  # Use the index from the original data\n",
    "    for model in item[\"response\"][\"Model\"]:\n",
    "        row[model['Name']] = model.get('Ranking', '')\n",
    "    csv_data.append(row)\n",
    "\n",
    "# Write to CSV\n",
    "csv_file = 'model_rankings.csv'\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in csv_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Data written to {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
