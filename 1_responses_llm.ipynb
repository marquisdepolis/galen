{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "folder_path = 'files'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "INSTRUCTION = config['instructions']\n",
    "F_NAME = config[\"name\"]\n",
    "\n",
    "# # Load the CSV file\n",
    "df = pd.read_excel('files/questions.xlsx') # pd.read_csv('questions.csv') \n",
    "# Save the original DataFrame as 'questions_original.xlsx'\n",
    "df.to_excel('files/questions_original.xlsx', index=False)\n",
    "\n",
    "# Check for duplicate questions\n",
    "duplicates = df.duplicated(subset=['Question'], keep=False)\n",
    "if duplicates.any():\n",
    "    print(\"Duplicates found. Removing duplicates.\")\n",
    "\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['Question'], keep='first')\n",
    "\n",
    "    # Save the modified DataFrame, overwriting the original 'questions.xlsx'\n",
    "    df.to_excel('files/questions.xlsx', index=False)\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Question', 'Response'])\n",
    "\n",
    "models = {\n",
    "    \"qwen-14b\": \"nomagick/qwen-14b-chat:f9e1ed25e2073f72ff9a3f46545d909b1078e674da543e791dec79218072ae70\",\n",
    "    # 3: \"01-ai/yi-34b:d83ccf090ccd5c7fe507ca302a558a850468293385d02bb807ee2753d802dd85\", # Not the chat model\n",
    "    \"falcon-40b\": \"joehoover/falcon-40b-instruct:7d58d6bddc53c23fa451c403b2b5373b1e0fa094e4e0d1b98c3d02931aa07173\",\n",
    "    \"yi-34b\": \"01-ai/yi-34b-chat:914692bbe8a8e2b91a4e44203e70d170c9c5ccc1359b283c84b0ec8d47819a46\",\n",
    "    \"mistral-7b\": \"mistralai/mistral-7b-instruct-v0.2:f5701ad84de5715051cb99d550539719f8a7fbcf65e0e62a3d1eb3f94720764e\",\n",
    "    \"llama2-70b\": \"meta/llama-2-70b-chat\",\n",
    "    \"openhermes2\": \"antoinelyset/openhermes-2.5-mistral-7b:d7ccd25700fb11c1787c25b580ac8d715d2b677202fe54b77f9b4a1eb7d73e2b\",\n",
    "    \"mixtral-instruct\": \"mistralai/mixtral-8x7b-instruct-v0.1:2b56576fcfbe32fa0526897d8385dd3fb3d36ba6fd0dbe033c72886b81ade93e\",\n",
    "    \"deepseek_33bq\": \"kcaverly/deepseek-coder-33b-instruct-gguf:ea964345066a8868e43aca432f314822660b72e29cab6b4b904b779014fe58fd\",\n",
    "    }\n",
    "\n",
    "prompt_for_qwen=\"\"\"<|im_start|>system\\nYou are a helpful assistant. {INSTRUCTION}.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "prompt_for_hermes = \"\"\"[\n",
    "{{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant. {INSTRUCTION}.\" \n",
    "}},\n",
    "{{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": {question}\n",
    "}}\n",
    "]\"\"\"\n",
    "\n",
    "# Iterate through each model\n",
    "for model_key, model_value in models.items():\n",
    "    responses = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        qn = row['Question']\n",
    "        question = json.dumps(qn)\n",
    "\n",
    "        if model_key == \"yi-34b\":  # Yi model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        if model_key == \"qwen-14b\":  # Qwen model\n",
    "            prompt = prompt_for_qwen.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        elif model_key == \"openhermes2\":  # Hermes model\n",
    "            prompt = prompt_for_hermes.format(INSTRUCTION=INSTRUCTION, question=question)\n",
    "        else:\n",
    "            plain_text_question = json.loads(question)\n",
    "            prompt = f\"You are a helpful assistant. {INSTRUCTION}. {plain_text_question}\"\n",
    "\n",
    "        try:\n",
    "            print(prompt)\n",
    "            output = replicate.run(\n",
    "                model_value,\n",
    "                input={\n",
    "                  \"debug\": False,\n",
    "                #   \"top_k\": 50,\n",
    "                  \"top_p\": 0.9,\n",
    "                  \"prompt\": prompt,\n",
    "                  \"temperature\": 0.7,\n",
    "                  \"max_new_tokens\": 500,\n",
    "                  \"min_new_tokens\": -1\n",
    "                }\n",
    "            )\n",
    "            response = \"\"\n",
    "            response_parts = []  # Initialize an empty list to collect string representations\n",
    "\n",
    "            for item in output:\n",
    "                item_str = str(item)  # Convert item to string\n",
    "                response += item_str if len(item_str) == 1 else f\" {item_str}\"\n",
    "                \n",
    "            response = response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "\n",
    "        new_row = pd.DataFrame({'Model': [model_key], 'Question': [qn], 'Response': [response]})\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "        if index % 20 == 0:  # Save every 10 questions, adjust as needed\n",
    "            results_df.to_excel(f'files/{F_NAME}_results_grouped_by_model.xlsx', index=False, sheet_name='Sheet1')\n",
    "            \n",
    "results_df.to_excel(f'files/{F_NAME}_results_grouped_by_model.xlsx', index=False, sheet_name='Sheet1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
